scala版：
/累计增量，streaming结合sql，建表查数据
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Time, Minutes, Seconds, StreamingContext, Duration}
import org.apache.spark.util.IntParam
import org.apache.spark.sql.SQLContext
import org.apache.spark.storage.StorageLevel

case class Record(word: String)

object SQLContextSingleton {
  @transient  private var instance: SQLContext = _
  def getInstance(sparkContext: SparkContext): SQLContext = {
    if (instance == null) {
      instance = new SQLContext(sparkContext)
    }
    instance
  }
}

val ssc = new StreamingContext(sc, Seconds(2))

val lines = ssc.textFileStream("/gpfs/fs01/user/root/data/spark141batch/work/data_xcdong")
val words = lines.flatMap(_.split(" "))
val windowDStream = words.window(Seconds(86400), Seconds(2))

windowDStream.foreachRDD((rdd: RDD[String], time: Time) => {
      val sqlContext = SQLContextSingleton.getInstance(rdd.sparkContext)
      import sqlContext.implicits._
      val wordsDataFrame = rdd.map(w => Record(w)).toDF()
      wordsDataFrame.registerTempTable("words")
      val wordCountsDataFrame = sqlContext.sql("select word, count(*) as total from words group by word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    })

ssc.start()
ssc.awaitTermination()


//python版，从目录下实时的读进来数据，建表查数据
from __future__ import print_function
import os
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SQLContext, Row

def getSqlContextInstance(sparkContext):
    if ('sqlContextSingletonInstance' not in globals()):
        globals()['sqlContextSingletonInstance'] = SQLContext(sparkContext)
    return globals()['sqlContextSingletonInstance']

def process(time, rdd):
        print("========= %s =========" % str(time))
        try:
            sqlContext = getSqlContextInstance(rdd.context)
            rowRdd = rdd.map(lambda w: Row(word=w))
            wordsDataFrame = sqlContext.createDataFrame(rowRdd)
            wordsDataFrame.registerTempTable("words")
            wordCountsDataFrame = sqlContext.sql("select word, count(*) as total from words group by word")
            wordCountsDataFrame.show()
        except:
            pass

ssc = StreamingContext(sc, 2)

lines = ssc.textFileStream("/opt/xcdong/data")
words = lines.flatMap(lambda line: line.split(" "))
windowDStream = words.window(86400, 2)
windowDStream.foreachRDD(process)
ssc.start()
ssc.awaitTermination()
